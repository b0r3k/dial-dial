%
%  Vzor bibliografické databáze pro automatickou tvorbu seznamu referencí
%  pomocí BibTeXu
%
%  Doporučený software pro správu databáze literatury formátu *.bib:
%    JabRef, http://jabref.sourceforge.net/
%
%  POZOR:
%
%    *  Písmena v názvech publikací, jež je třeba tisknout vždy velká, se musejí psát
%       do složených závorek ({T}hailand, {HIV})
%
%  ===========================================================================

@article{OSHAUGHNESSY198855,
  title    = {Diphone speech synthesis},
  journal  = {Speech Communication},
  volume   = {7},
  number   = {1},
  pages    = {55-65},
  year     = {1988},
  issn     = {0167-6393},
  doi      = {https://doi.org/10.1016/0167-6393(88)90021-0},
  url      = {https://www.sciencedirect.com/science/article/pii/0167639388900210},
  author   = {Douglas O'Shaughnessy and Louis Barbeau and David Bernardi and Danièle Archambault},
  keywords = {Speech, synthesis, diphone, text-to-speech, coarticulation, syllable, interpolation, coding, linear prediction, memory, concatenation},
  abstract = {Text-to-speech synthesis requires two steps: linguistic processing (to convert text into phonemes and intonation parameters) and simulation of speech production (to generate the speech waveform). We review different methods for the second task, emphasizing the advantages and disadvantages of the linear predictive (LPC) diphone approach. Diphones require more memory represent all possible spectral transitions between pairs of phonemes, but they directly capture many of the coarticulation effects that must otherwise be modeled in phonemic synthesis. Relatively simple interpolation is allowed due to the similarity of spectra at diphone boundaries.
Zusammenfassung
Sprachsynthese vom Text bis him zum akustischen Signal benötigt zwei Schritte: die linguistische Verarbeitung, d.h., die Umwandlung des Textes in eine Folge von Phonemen und prosodischen Parametern, und die Simulation der akustischen Sprachproduktion, d.h., die Generierung des Sprachsignals. Wir untersuchen verschiedene Verfahren für den zweiten Schritt, wobei die Vor- und Nachteile der Synthese unter Verwendung von Diphonen sowie der linearen Prädiktion eingehend diskutiert werden. Um alle möglichen spektralen Übergänge zwischen Phonempaaren darstellen zu können, benötigen Diphone mehr Speicherplatz; andererseits werden durch sie viele Koartikulationseffekte in den gespeicherten Daten abgefangen, die bei einer Synthese auf der Basis von Phonemen durch Regeln nachgebildet werden müßten. Die Interpolation zwischen benachbarten Diphonen wird verhältnismäßig einfach, da die Spektren beiderseits der Schnittstelle ähnlich verlaufen.
Résumé
La synthèse texte-parole nécessite une étape de traitement linguistique et une étape de simulation acoustique. D'abord, le texte est transformé en phonèmes et paramètres intonatifs, ensuite la production de parole est simulée afin de générer des signaux acoustiques. Nous étudions plusieurs méthodes pour accomplir la deuxième tàche, en insistant sur les avantages et désavantages d'une approche par diphones basée sur la prédiction linéaire. Les diphones nécessitent plus de mémoire ordinateur afin de representer toutes les transitions possibles eentre couples de phonèmes, mais ils incorporent en grande partie les effets de coarticulation qui sinon devraient étre modélisés lors de la synthèse phonémique. Les règles d'interpolation sont simples parce qu'au niveau spectral les frontières entre diphones sont similaires.}
}


@misc{li2020universal,
  title         = {Universal Phone Recognition with a Multilingual Allophone System},
  author        = {Xinjian Li and Siddharth Dalmia and Juncheng Li and Matthew Lee and Patrick Littell and Jiali Yao and Antonios Anastasopoulos and David R. Mortensen and Graham Neubig and Alan W Black and Florian Metze},
  year          = {2020},
  eprint        = {2002.11800},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@article{glass_challenges_1999,
  author = {Glass, James},
  year   = {1999},
  month  = {10},
  pages  = {},
  title  = {Challenges For Spoken Dialogue Systems}
}


@online{mvcr_cetnost_2018,
  title   = {Četnost jmen a příjmení - Ministerstvo vnitra České republiky},
  url     = {https://web.archive.org/web/20180922062314/https://www.mvcr.cz/clanek/cetnost-jmen-a-prijmeni.aspx},
  urldate = {2021-07-20},
  year    = {2018},
  author  = {{MVČR}}
}


@misc{mason_ahnouncing_2019,
  title    = {Ahnouncing adaptive psell chekc ni {Watson} {Assistant}},
  url      = {https://medium.com/ibm-watson/ahnouncing-psell-chekc-ni-watson-assistant-2d8764cb36d9},
  abstract = {Qukic, casualy written messags have beocme the norm for comunication between individuals tody (think iMessage, WhatsApp, SMS, Slack, Skype…},
  language = {en},
  urldate  = {2021-07-19},
  journal  = {Medium},
  author   = {Mason, Mitchell},
  month    = {04},
  year     = {2019}
}


@misc{zhuang2020comprehensive,
  title         = {A Comprehensive Survey on Transfer Learning},
  author        = {Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
  year          = {2020},
  eprint        = {1911.02685},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@inproceedings{bosertraining,
  title     = {A training algorithm for optimal margin classifiers},
  author    = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory},
  pages     = {144--152},
  year      = {1996},
  month     = {08}
}


@misc{hospedales2020metalearning,
  title         = {Meta-Learning in Neural Networks: A Survey},
  author        = {Timothy Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},
  year          = {2020},
  eprint        = {2004.05439},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}


@article{He_2021,
  title     = {AutoML: A survey of the state-of-the-art},
  volume    = {212},
  issn      = {0950-7051},
  url       = {http://dx.doi.org/10.1016/j.knosys.2020.106622},
  doi       = {10.1016/j.knosys.2020.106622},
  journal   = {Knowledge-Based Systems},
  publisher = {Elsevier BV},
  author    = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  year      = {2021},
  month     = {Jan},
  pages     = {106622}
}


@inproceedings{arora-etal-2020-hint3,
  title     = {{HINT}3: Raising the bar for Intent Detection in the Wild},
  author    = {Arora, Gaurav and Jain, Chirag and Chaturvedi, Manas and Modi, Krupal},
  booktitle = {Proceedings of the First Workshop on Insights from Negative Results in NLP},
  month     = {11},
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.insights-1.16},
  doi       = {10.18653/v1/2020.insights-1.16},
  pages     = {100--105},
  abstract  = {Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data.}
}


@misc{potdar_watson_2021,
  title      = {Watson {Assistant}: {Under} the hood},
  shorttitle = {Watson {Assistant}},
  url        = {https://medium.com/ibm-watson/under-the-hood-all-the-natural-language-understanding-technology-that-makes-watson-assistant-fb8aaaca2f16},
  abstract   = {All the natural language understanding technology that makes Watson Assistant so powerful},
  language   = {en},
  urldate    = {2021-07-19},
  journal    = {Medium},
  author     = {Potdar, Saloni},
  month      = {05},
  year       = {2021}
}


@misc{noauthor_watson_2020,
  title    = {Watson {Assistant} improves intent detection accuracy, leads against {AI} vendors cited in published study},
  url      = {https://www.ibm.com/blogs/watson/2020/12/watson-assistant-improves-intent-detection-accuracy-leads-against-ai-vendors-cited-in-published-study/},
  abstract = {Watson Assistant has a new and improved intent detection algorithm, which is more accurate versus commercial and open-source solutions.},
  language = {en-US},
  urldate  = {2021-07-19},
  journal  = {Watson Blog},
  month    = {12},
  year     = {2020},
  author   = {IBM}
}


@misc{qi2021benchmarking,
  title         = {Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations},
  author        = {Haode Qi and Lin Pan and Atin Sood and Abhishek Shah and Ladislav Kunc and Mo Yu and Saloni Potdar},
  year          = {2021},
  eprint        = {2012.03929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@online{akbulut_common_2020,
  title      = {Common conversational design patterns with Watson Assistant},
  url        = {https://medium.com/@mbambamba/common-conversational-design-patterns-with-watson-assistant-f99c4e1dc358},
  abstract   = {Design your chatbot to go beyond the happy path on day one},
  titleaddon = {Medium},
  author     = {Akbulut, Burak},
  urldate    = {2021-07-19},
  date       = {2020-04-06},
  year       = {2020},
  langid     = {english}
}


@misc{wa_universal_model,
  title   = {Adding support for global audiences},
  url     = {https://cloud.ibm.com/docs/assistant?topic=assistant-assistant-language},
  author  = {IBM},
  urldate = {2021-07-19},
  year    = {2021}
}


@misc{wa_languages,
  title   = {Supported languages},
  url     = {https://cloud.ibm.com/docs/assistant?topic=assistant-language-support},
  author  = {IBM},
  urldate = {2021-07-19},
  year    = {2021}
}


@misc{wa_about,
  title   = {About Watson Assistant},
  url     = {https://cloud.ibm.com/docs/assistant?topic=assistant-index},
  author  = {IBM},
  urldate = {2021-07-19},
  year    = {2021}
}


@misc{google_github_tacotron,
  title   = {Audio samples related to {Tacotron}, an end-to-end speech synthesis system by {Google}.},
  url     = {https://google.github.io/tacotron/},
  author  = {Google Tacotron},
  urldate = {2021-07-18},
  year    = {2021}
}


@misc{wang2017tacotron,
  title         = {Tacotron: Towards End-to-End Speech Synthesis},
  author        = {Yuxuan Wang and RJ Skerry-Ryan and Daisy Stanton and Yonghui Wu and Ron J. Weiss and Navdeep Jaitly and Zongheng Yang and Ying Xiao and Zhifeng Chen and Samy Bengio and Quoc Le and Yannis Agiomyrgiannakis and Rob Clark and Rif A. Saurous},
  year          = {2017},
  eprint        = {1703.10135},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@article{oord_wavenet_2016,
  title      = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
  shorttitle = {{WaveNet}},
  url        = {http://arxiv.org/abs/1609.03499},
  abstract   = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  urldate    = {2021-07-18},
  journal    = {arXiv:1609.03499 [cs]},
  author     = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  month      = sep,
  year       = {2016},
  note       = {arXiv: 1609.03499},
  keywords   = {Computer Science - Machine Learning, Computer Science - Sound}
}


@inproceedings{vocaine_2015,
  title     = {Vocaine the Vocoder and Applications in Speech Synthesis},
  author    = {Yannis Agiomyrgiannakis},
  year      = {2015},
  booktitle = {ICASSP}
}


@misc{google_tts,
  title    = {Standard and {WaveNet} voices {Cloud} {Text}-to-{Speech} {Documentation}},
  url      = {https://cloud.google.com/text-to-speech/docs/wavenet},
  language = {en},
  urldate  = {2021-07-18},
  year     = {2021},
  journal  = {Google Cloud},
  author   = {Google}
}


@article{wen_stochastic_2015,
  title        = {Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking},
  url          = {http://arxiv.org/abs/1508.01755},
  abstract     = {The natural language generation ({NLG}) component of a spoken dialogue system ({SDS}) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add significantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predefined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems.},
  journaltitle = {{arXiv}:1508.01755 [cs]},
  author       = {Wen, Tsung-Hsien and Gasic, Milica and Kim, Dongho and Mrksic, Nikola and Su, Pei-Hao and Vandyke, David and Young, Steve},
  urldate      = {2021-07-17},
  date         = {2015-08-07},
  year         = {2015},
  eprinttype   = {arxiv},
  eprint       = {1508.01755},
  keywords     = {Computer Science - Computation and Language}
}


@book{teich_grammars_1999,
  title     = {Systemic functional grammar in natural language generation: linguistic description and computational representation},
  author    = {Teich, Elke},
  year      = {1999},
  isbn      = {978-0-304-70168-1},
  pagetotal = {272},
  publisher = {Continuum; 1st edition (May 11, 1999)}
}


@article{su_reward_2015,
  title        = {Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems},
  url          = {http://arxiv.org/abs/1508.03391},
  abstract     = {Statistical spoken dialogue systems have the attractive property of being able to be optimised from data via interactions with real users. However in the reinforcement learning paradigm the dialogue manager (agent) often requires significant time to explore the state-action space to learn to behave in a desirable manner. This is a critical issue when the system is trained on-line with real users where learning costs are expensive. Reward shaping is one promising technique for addressing these concerns. Here we examine three recurrent neural network ({RNN}) approaches for providing reward shaping information in addition to the primary (task-orientated) environmental feedback. These {RNNs} are trained on returns from dialogues generated by a simulated user and attempt to diffuse the overall evaluation of the dialogue back down to the turn level to guide the agent towards good behaviour faster. In both simulated and real user scenarios these {RNNs} are shown to increase policy learning speed. Importantly, they do not require prior knowledge of the user's goal.},
  journaltitle = {{arXiv}:1508.03391 [cs]},
  author       = {Su, Pei-Hao and Vandyke, David and Gasic, Milica and Mrksic, Nikola and Wen, Tsung-Hsien and Young, Steve},
  urldate      = {2021-07-17},
  date         = {2015-08-18},
  year         = {2015},
  eprinttype   = {arxiv},
  eprint       = {1508.03391},
  keywords     = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}


@article{zhang_pushing_2020,
  title        = {Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition},
  url          = {http://arxiv.org/abs/2010.10504},
  abstract     = {We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on {LibriSpeech} utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with {SpecAugment} using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates ({WERs}) 1.4\%/2.6\% on the {LibriSpeech} test/test-other sets against the current state-of-the-art {WERs} 1.7\%/3.3\%.},
  journaltitle = {{arXiv}:2010.10504 [cs, eess]},
  author       = {Zhang, Yu and Qin, James and Park, Daniel S. and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V. and Wu, Yonghui},
  urldate      = {2021-07-17},
  date         = {2020-10-20},
  year         = {2020},
  eprinttype   = {arxiv},
  eprint       = {2010.10504},
  note         = {version: 1},
  keywords     = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing}
}


@article{mezza_iso-standard_2018,
  title        = {{ISO}-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents},
  url          = {http://arxiv.org/abs/1806.04327},
  abstract     = {Dialogue Act ({DA}) tagging is crucial for spoken language understanding systems, as it provides a general representation of speakers' intents, not bound to a particular dialogue system. Unfortunately, publicly available data sets with {DA} annotation are all based on different annotation schemes and thus incompatible with each other. Moreover, their schemes often do not cover all aspects necessary for open-domain human-machine interaction. In this paper, we propose a methodology to map several publicly available corpora to a subset of the {ISO} standard, in order to create a large task-independent training corpus for {DA} classification. We show the feasibility of using this corpus to train a domain-independent {DA} tagger testing it on out-of-domain conversational data, and argue the importance of training on multiple corpora to achieve robustness across different {DA} categories.},
  journaltitle = {{arXiv}:1806.04327 [cs]},
  author       = {Mezza, Stefano and Cervone, Alessandra and Tortoreto, Giuliano and Stepanov, Evgeny A. and Riccardi, Giuseppe},
  urldate      = {2021-07-17},
  date         = {2018-06-12},
  year         = {2018},
  eprinttype   = {arxiv},
  eprint       = {1806.04327},
  keywords     = {Computer Science - Computation and Language}
}

@article{liu_multi-task_2019,
  title        = {Multi-Task Deep Neural Networks for Natural Language Understanding},
  url          = {http://arxiv.org/abs/1901.11504},
  abstract     = {In this paper, we present a Multi-Task Deep Neural Network ({MT}-{DNN}) for learning representations across multiple natural language understanding ({NLU}) tasks. {MT}-{DNN} not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. {MT}-{DNN} extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as {BERT} (Devlin et al., 2018). {MT}-{DNN} obtains new state-of-the-art results on ten {NLU} tasks, including {SNLI}, {SciTail}, and eight out of nine {GLUE} tasks, pushing the {GLUE} benchmark to 82.7\% (2.2\% absolute improvement). We also demonstrate using the {SNLI} and {SciTail} datasets that the representations learned by {MT}-{DNN} allow domain adaptation with substantially fewer in-domain labels than the pre-trained {BERT} representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.},
  journaltitle = {{arXiv}:1901.11504 [cs]},
  author       = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  urldate      = {2021-07-17},
  date         = {2019-05-29},
  year         = {2019},
  eprinttype   = {arxiv},
  eprint       = {1901.11504},
  keywords     = {Computer Science - Computation and Language}
}


@article{brown_language_2020,
  title        = {Language Models are Few-Shot Learners},
  url          = {http://arxiv.org/abs/2005.14165},
  abstract     = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
  journaltitle = {{arXiv}:2005.14165 [cs]},
  author       = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  urldate      = {2021-07-17},
  date         = {2020-07-22},
  year         = {2020},
  month        = {07},
  eprinttype   = {arxiv},
  eprint       = {2005.14165},
  note         = {version: 4},
  keywords     = {Computer Science - Computation and Language}
}


@article{young_augmenting_2018,
  title        = {Augmenting End-to-End Dialog Systems with Commonsense Knowledge},
  url          = {http://arxiv.org/abs/1709.05453},
  abstract     = {Building dialog agents that can converse naturally with humans is a challenging yet intriguing problem of artificial intelligence. In open-domain human-computer conversation, where the conversational agent is expected to respond to human responses in an interesting and engaging way, commonsense knowledge has to be integrated into the model effectively. In this paper, we investigate the impact of providing commonsense knowledge about the concepts covered in the dialog. Our model represents the first attempt to integrating a large commonsense knowledge base into end-to-end conversational models. In the retrieval-based scenario, we propose the Tri-{LSTM} model to jointly take into account message and commonsense for selecting an appropriate response. Our experiments suggest that the knowledge-augmented models are superior to their knowledge-free counterparts in automatic evaluation.},
  journaltitle = {{arXiv}:1709.05453 [cs]},
  author       = {Young, Tom and Cambria, Erik and Chaturvedi, Iti and Huang, Minlie and Zhou, Hao and Biswas, Subham},
  urldate      = {2021-07-17},
  year         = {2018},
  month        = {02},
  date         = {2018-02-12},
  eprinttype   = {arxiv},
  eprint       = {1709.05453},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language}
}


@inproceedings{turn_taking_taxonomy_2015,
  author = {Khouzaimi, Hatim and Laroche, Romain and Lefèvre, Fabrice},
  year   = {2015},
  month  = {09},
  pages  = {},
  title  = {Turn-taking phenomena in incremental dialogue systems},
  doi    = {10.18653/v1/D15-1216}
}

@article{khouzaimi_turn-taking_2016,
  title  = {Turn-taking enhancement in spoken dialogue systems with reinforcement learning},
  pages  = {167},
  author = {Khouzaimi, Hatim},
  langid = {english},
  month  = {06},
  year   = {2016}
}


@article{faraj_facially_2021,
  title    = {Facially expressive humanoid robotic face},
  volume   = {9},
  issn     = {2468-0672},
  url      = {https://www.sciencedirect.com/science/article/pii/S2468067220300262},
  doi      = {https://doi.org/10.1016/j.ohx.2020.e00117},
  abstract = {Realistic humanoid robots have emerged in the last two decades but the emotional intelligence of these machines has been limited. To teach humanoids how to emotionally communicate with humans, researchers have been increasingly relying on machine learning algorithms. While the software used to implement machine learning algorithms is largely open source, facially expressive humanoid robots are expensive and inaccessible to most people, thus limiting the number of researchers in this field. This paper aims to aid potential artificial intelligence researchers by providing a relatively inexpensive, open-source robot that can serve as a platform for research into emotional communication between humans and machines. Eva, the robot described in this paper, is an adult-sized humanoid head that can emulate human facial expressions, head movements, and speech through the use of 25 muscles, including 12 facial muscles that can produce a maximum skin displacement of 15 mm.},
  pages    = {e00117},
  journal  = {{HardwareX}},
  author   = {Faraj, Zanwar and Selamet, Mert and Morales, Carlos and Torres, Patricio and Hossain, Maimuna and Chen, Boyuan and Lipson, Hod},
  year     = {2021},
  keywords = {Artificial intelligence, Emotions, Face robot, Facial expressions, Humanoid}
}


@online{voicebotai_2021,
  author     = {voicebot.ai},
  title      = {Voice Assistant Timeline},
  url        = {http://voicebot.ai/voice-assistant-history-timeline/},
  abstract   = {Updated January 2021.},
  titleaddon = {Voicebot.ai},
  urldate    = {2021-07-17},
  year       = {2021},
  month      = {01},
  langid     = {american}
}


@book{mctear_conversational_2020,
  title      = {Conversational {AI}: Dialogue Systems, Conversational Agents, and Chatbots},
  isbn       = {978-1-63639-032-1},
  shorttitle = {Conversational {AI}},
  abstract   = {This book provides a comprehensive introduction to Conversational {AI}. While the idea of interacting with a computer using voice or text goes back a long way, it is only in recent years that this idea has become a reality with the emergence of digital personal assistants, smart speakers, and chatbots. Advances in {AI}, particularly in deep learning, along with the availability of massive computing power and vast amounts of data, have led to a new generation of dialogue systems and conversational interfaces. Current research in Conversational {AI} focuses mainly on the application of machine learning and statistical data-driven approaches to the development of dialogue systems. However, it is important to be aware of previous achievements in dialogue technology and to consider to what extent they might be relevant to current research and development. Three main approaches to the development of dialogue systems are reviewed: rule-based systems that are handcrafted using best practice guidelines; statistical data-driven systems based on machine learning; and neural dialogue systems based on end-to-end learning. Evaluating the performance and usability of dialogue systems has become an important topic in its own right, and a variety of evaluation metrics and frameworks are described. Finally, a number of challenges for future research are considered, including: multimodality in dialogue systems, visual dialogue; data efficient dialogue model learning; using knowledge graphs; discourse and dialogue phenomena; hybrid approaches to dialogue systems development; dialogue with social robots and in the Internet of Things; and social and ethical issues.},
  pagetotal  = {253},
  publisher  = {Morgan \& Claypool Publishers},
  author     = {{McTear}, Michael},
  date       = {2020-10-30},
  year       = {2020},
  month      = {10},
  langid     = {english},
  note       = {Google-Books-{ID}: {kGAMEAAAQBAJ}},
  keywords   = {Computers / Artificial Intelligence / General, Computers / Artificial Intelligence / Natural Language Processing, Language Arts \& Disciplines / Linguistics / General}
}


@article{gao_neural_2019,
  title        = {Neural Approaches to Conversational {AI}},
  url          = {http://arxiv.org/abs/1809.08267},
  abstract     = {The present paper surveys neural approaches to conversational {AI} that have been developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies.},
  journaltitle = {{arXiv}:1809.08267 [cs]},
  author       = {Gao, Jianfeng and Galley, Michel and Li, Lihong},
  urldate      = {2021-07-17},
  date         = {2019-09-10},
  year         = {2019},
  month        = {09},
  eprinttype   = {arxiv},
  eprint       = {1809.08267},
  keywords     = {Computer Science - Computation and Language}
}


@inproceedings{theory_practice_coroutines,
  author = {Prokopec, Aleksandar and Liu, Fengyun},
  year   = {2018},
  month  = {07},
  pages  = {},
  title  = {{Theory} and {Practice} of {Coroutines} with {Snapshots}},
  doi    = {10.4230/LIPIcs.ECOOP.2018.3}
}

@online{android_blog,
  author     = {David Winer},
  title      = {{Android’s} {commitment} to {Kotlin}},
  url        = {https://android-developers.googleblog.com/2019/12/androids-commitment-to-kotlin.html},
  abstract   = {Posted by David Winer , Kotlin Product Manager      When we announced Kotlin as a supported language for Android, there was a tremendous ...},
  titleaddon = {Android Developers Blog},
  year       = {2019},
  month      = {12},
  urldate    = {2021-07-17},
  langid     = {english}
}

@article{prof_tejinder_singh_hotspot_2014,
  author  = {Singh, Dr. Tejinder},
  year    = {2014},
  month   = {10},
  pages   = {2350-1294},
  title   = {The Hotspot Java Virtual Machine: Memory and Architecture},
  volume  = {1},
  journal = {International Journal of Allied Practice, Research and Review},
  doi     = {10.13140/2.1.2442.7206}
}

@incollection{ekstein_czech_2019,
  address    = {Cham},
  title      = {Czech {Text} {Processing} with {Contextual} {Embeddings}: {POS} {Tagging}, {Lemmatization}, {Parsing} and {NER}},
  volume     = {11697},
  isbn       = {978-3-030-27946-2 978-3-030-27947-9},
  shorttitle = {Czech {Text} {Processing} with {Contextual} {Embeddings}},
  url        = {http://link.springer.com/10.1007/978-3-030-27947-9_12},
  abstract   = {Contextualized embeddings, which capture appropriate word meaning depending on context, have recently been proposed. We evaluate two methods for precomputing such embeddings, BERT and Flair, on four Czech text processing tasks: part-of-speech (POS) tagging, lemmatization, dependency parsing and named entity recognition (NER). The ﬁrst three tasks, POS tagging, lemmatization and dependency parsing, are evaluated on two corpora: the Prague Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We report state-of-the-art results for the above mentioned tasks and corpora.},
  language   = {en},
  urldate    = {2021-07-10},
  booktitle  = {Text, {Speech}, and {Dialogue}},
  publisher  = {Springer International Publishing},
  author     = {Straka, Milan and Straková, Jana and Hajič, Jan},
  editor     = {Ekštein, Kamil},
  year       = {2019},
  doi        = {10.1007/978-3-030-27947-9_12},
  note       = {Series Title: Lecture Notes in Computer Science},
  pages      = {137--150}
}

@book{jurafsky_slp_2020,
  author   = {Jurafsky, Daniel and Martin, James},
  year     = {2020},
  month    = {12},
  url      = {https://web.stanford.edu/~jurafsky/slp3/ed3book_dec302020.pdf},
  urldate  = {2021-07-10},
  pages    = {},
  language = {en},
  title    = {{Speech} and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition}},
  volume   = {3}
}

@article{Levenshtein1965BinaryCC,
  title   = {Binary codes capable of correcting deletions, insertions, and reversals},
  author  = {V. Levenshtein},
  journal = {Soviet physics. Doklady},
  year    = {1965},
  volume  = {10},
  pages   = {707-710}
}

@inproceedings{tang_howl_2020,
  address    = {Online},
  title      = {Howl: {A} {Deployed}, {Open}-{Source} {Wake} {Word} {Detection} {System}},
  shorttitle = {Howl},
  url        = {https://aclanthology.org/2020.nlposs-1.9},
  doi        = {10.18653/v1/2020.nlposs-1.9},
  abstract   = {We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on GSC and our own freely available wake word detection dataset, built from MCV. One of our models is deployed in Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized, open-source wake word detection toolkit with a web browser deployment target. Our codebase is at howl.ai.},
  urldate    = {2021-07-09},
  booktitle  = {Proceedings of {Second} {Workshop} for {NLP} {Open} {Source} {Software} ({NLP}-{OSS})},
  publisher  = {Association for Computational Linguistics},
  author     = {Tang, Raphael and Lee, Jaejun and Razi, Afsaneh and Cambre, Julia and Bicking, Ian and Kaye, Jofish and Lin, Jimmy},
  month      = nov,
  year       = {2020},
  pages      = {61--65}
}

@book{brooks_handbook_2011,
  title     = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
  isbn      = {978-1-4200-7942-5},
  abstract  = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisherie},
  language  = {en},
  publisher = {CRC Press},
  author    = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  month     = may,
  year      = {2011},
  note      = {Google-Books-ID: qfRsAIKZ4rIC},
  keywords  = {Mathematics / Probability \& Statistics / General}
}


@article{williams_dialog_2016,
  title      = {The {Dialog} {State} {Tracking} {Challenge} {Series}: {A} {Review}},
  volume     = {7},
  issn       = {2152-9620},
  shorttitle = {The {Dialog} {State} {Tracking} {Challenge} {Series}},
  url        = {https://journals.uic.edu/ojs/index.php/dad/article/view/10729},
  doi        = {10.5087/dad.2016.301},
  abstract   = {In a spoken dialog system, dialog state tracking refers to the task of correctly inferring the state of the conversation – such as the user’s goal – given all of the dialog history up to that turn. Dialog state tracking is crucial to the success of a dialog system, yet until recently there were no common resources, hampering progress. The Dialog State Tracking Challenge series of 3 tasks introduced the ﬁrst shared testbed and evaluation metrics for dialog state tracking, and has underpinned three key advances in dialog state tracking: the move from generative to discriminative models; the adoption of discriminative sequential techniques; and the incorporation of the speech recognition results directly into the dialog state tracker. This paper reviews this research area, covering both the challenge tasks themselves and summarizing the work they have enabled.},
  language   = {en},
  number     = {3},
  urldate    = {2021-07-09},
  journal    = {Dialogue \& Discourse},
  author     = {Williams, Jason D. and Raux, Antoine and Henderson, Matthew},
  month      = apr,
  year       = {2016},
  pages      = {4--33}
}
